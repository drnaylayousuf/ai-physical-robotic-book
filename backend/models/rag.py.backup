from typing import List, Dict, Optional, Tuple
from pydantic import BaseModel
import logging
from fastapi import HTTPException
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import PointStruct, Filter, FieldCondition, MatchValue
from ..config.settings import settings
from ..utils.embeddings import EmbeddingService
from ..utils.monitoring import monitoring

logger = logging.getLogger(__name__)

class RAGResponse(BaseModel):
    response: str
    sources: List[Dict]
    references: List[str]

class RAGModel:
    """
    RAG (Retrieval-Augmented Generation) model implementation
    Implements the clean → chunk → embed → store → retrieve → generate flow
    """

    def __init__(self):
        self.embedding_service = EmbeddingService()
        # Initialize Qdrant client
        try:
            # Check if we have an API key for authentication
            if settings.QDRANT_API_KEY:
                self.qdrant_client = QdrantClient(
                    url=settings.QDRANT_URL,
                    api_key=settings.QDRANT_API_KEY,
                    timeout=10
                )
            else:
                self.qdrant_client = QdrantClient(
                    url=settings.QDRANT_URL,
                    timeout=10
                )
            # Test the connection
            self.qdrant_client.get_collections()
            logger.info(f"Successfully connected to Qdrant at {settings.QDRANT_URL}")

            # Verify collection exists before proceeding
            collection_name = settings.QDRANT_COLLECTION_NAME
            try:
                collection_info = self.qdrant_client.get_collection(collection_name)
                logger.info(f"Verified collection '{collection_name}' exists with {collection_info.points_count} points")
            except Exception as e:
                logger.warning(f"Collection '{collection_name}' does not exist yet: {e}. It will be created when data is ingested.")

        except Exception as e:
            logger.error(f"Failed to connect to Qdrant at {settings.QDRANT_URL}: {e}")
            raise ConnectionError(f"Could not connect to Qdrant: {e}")

        # Initialize Gemini generative model
        self.generative_model = None
        if settings.GEMINI_API_KEY:
            try:
                import google.generativeai as genai
                genai.configure(api_key=settings.GEMINI_API_KEY)
                self.generative_model = genai.GenerativeModel(settings.GEMINI_MODEL)
                logger.info(f"Gemini generative model {settings.GEMINI_MODEL} configured")
            except Exception as e:
                logger.error(f"Failed to configure Gemini generative model: {e}")

    async def clean_text(self, text: str) -> str:
        """
        Clean and normalize text content
        """
        # Remove extra whitespace
        text = ' '.join(text.split())
        # Remove special characters if needed
        # Add more cleaning steps as needed
        return text.strip()

    async def chunk_text(self, text: str, chunk_size: int = None, overlap: int = None) -> List[str]:
        """
        Split text into chunks with specified size and overlap
        """
        if chunk_size is None:
            chunk_size = settings.CHUNK_SIZE
        if overlap is None:
            overlap = settings.CHUNK_OVERLAP

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)

            # Move start position by chunk_size - overlap to create overlap
            start = end - overlap if end < len(text) else end

            # If we've reached the end, break
            if start >= len(text):
                break

        return chunks

    async def embed_chunks(self, chunks: List[str]) -> List[List[float]]:
        """
        Generate embeddings for text chunks
        """
        embeddings = []
        for chunk in chunks:
            embedding = await self.embedding_service.generate_embedding(chunk)
            embeddings.append(embedding)
        return embeddings

    async def store_chunks(self, chunks: List[str], embeddings: List[List[float]], collection_name: str = None) -> List[str]:
        """
        Store chunks and embeddings in vector database (Qdrant)
        """
        if collection_name is None:
            collection_name = settings.QDRANT_COLLECTION_NAME

        # Check if collection exists, create if it doesn't
        try:
            collection_info = self.qdrant_client.get_collection(collection_name)
            # Verify the vector size matches
            expected_size = len(embeddings[0]) if embeddings else self.embedding_service.get_embedding_size()
            # Access the vector size properly - it might be in different structures depending on Qdrant version
            if hasattr(collection_info.config.params, 'vectors'):
                # For newer Qdrant versions with named vectors
                if isinstance(collection_info.config.params.vectors, dict):
                    # If vectors is a dict, get the first vector configuration
                    first_vector_name = next(iter(collection_info.config.params.vectors))
                    actual_size = collection_info.config.params.vectors[first_vector_name].size
                else:
                    # If vectors is a VectorParams object
                    actual_size = collection_info.config.params.vectors.size
            elif hasattr(collection_info.config.params, 'size'):
                # For older Qdrant versions or default vector configuration
                actual_size = collection_info.config.params.size
            else:
                # Fallback to the embedding size
                actual_size = expected_size

            if actual_size != expected_size:
                logger.warning(f"Collection {collection_name} has vector size {actual_size}, but expected {expected_size}")
        except:
            # Create collection if it doesn't exist
            self.qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(size=len(embeddings[0]) if embeddings else self.embedding_service.get_embedding_size(), distance=models.Distance.COSINE)
            )
            logger.info(f"Created new Qdrant collection: {collection_name}")

        # Prepare points for insertion
        points = []
        chunk_ids = []

        # Get the current count of points to start from the next ID
        try:
            current_count = self.qdrant_client.count(collection_name=collection_name).count
        except:
            current_count = 0

        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunk_id = current_count + i + 1  # Use sequential integer IDs
            chunk_ids.append(str(chunk_id))  # Store as string for return

            points.append(
                PointStruct(
                    id=chunk_id,  # Use integer ID as required by Qdrant
                    vector=embedding,
                    payload={
                        "content": chunk,
                        "collection_name": collection_name,
                        "chunk_index": i,
                        "original_chunk_id": f"{collection_name}_chunk_{i}"  # Keep original ID in payload
                    }
                )
            )

        # Insert points into Qdrant
        self.qdrant_client.upsert(
            collection_name=collection_name,
            points=points
        )

        logger.info(f"Stored {len(chunk_ids)} chunks in collection {collection_name}")
        return chunk_ids

    async def retrieve_context(self, query: str, top_k: int = 5, collection_name: str = None) -> List[Dict]:
        """
        Retrieve relevant context from vector database based on query
        Ensure retrieved chunks return: chunk_id, content, score/confidence, and source metadata
        """
        if collection_name is None:
            collection_name = settings.QDRANT_COLLECTION_NAME

        # Verify the collection exists
        try:
            collection_info = self.qdrant_client.get_collection(collection_name)
            logger.debug(f"Collection {collection_name} exists with {collection_info.points_count} points")
        except Exception as e:
            logger.error(f"Collection {collection_name} does not exist: {e}")
            return []

        # Generate embedding for the query
        query_embedding = await self.embedding_service.generate_embedding(query)

        # Query in Qdrant (using the newer query_points method)
        try:
            from qdrant_client.http import models
            search_results = self.qdrant_client.query_points(
                collection_name=collection_name,
                query=query_embedding,
                limit=top_k,
                with_payload=True
            ).points
        except Exception as e:
            logger.error(f"Error searching in Qdrant: {e}")
            return []

        results = []
        for result in search_results:
            # Extract source metadata from the payload
            source_metadata = result.payload.get("source", result.payload.get("collection_name", collection_name))

            results.append({
                "chunk_id": str(result.id),  # Convert to string to ensure consistency
                "content": result.payload.get("content", ""),
                "score": result.score,  # Using 'score' instead of 'confidence' to match requirements
                "source": source_metadata
            })

        logger.info(f"Retrieved {len(results)} chunks from Qdrant collection {collection_name}")
        return results

    def _calculate_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """
        Calculate cosine similarity between two embeddings
        """
        # Simplified similarity calculation
        # In a real implementation, this would use proper cosine similarity
        dot_product = sum(a * b for a, b in zip(embedding1, embedding2))
        magnitude1 = sum(a * a for a in embedding1) ** 0.5
        magnitude2 = sum(b * b for b in embedding2) ** 0.5

        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        return dot_product / (magnitude1 * magnitude2)

    async def generate_response(self, query: str, context: List[Dict], max_tokens: int = 500) -> str:
        """
        Generate response based on query and retrieved context using Gemini
        """
        if not self.generative_model:
            raise ValueError("Gemini model is not configured properly. Check GEMINI_API_KEY in environment.")

        # Prepare the context for the LLM
        context_texts = [item["content"] for item in context]
        context_str = "\n\n".join(context_texts)

        if not context_str.strip():
            # If no context is provided, respond with a specific message
            prompt = f"Use the following context to answer the question.\nIf the context does not contain enough information, say \"The book does not provide details about this topic.\"\n\nQuestion: {query}\n\nContext: No relevant context found in the book."
        else:
            prompt = f"Use the following context to answer the question.\nIf the context does not contain enough information, say \"The book does not provide details about this topic.\"\n\nQuestion: {query}\n\nContext: {context_str}"

        try:
            # Generate response using Gemini
            response = await self.generative_model.generate_content_async(
                prompt,
                generation_config={
                    "temperature": 0.7,
                    "max_output_tokens": max_tokens,
                    "candidate_count": 1
                }
            )

            # Extract the text from the response
            if response.candidates and len(response.candidates) > 0:
                return response.candidates[0].content.parts[0].text
            else:
                return "The book does not provide details about this topic."

        except Exception as e:
            error_msg = str(e).lower()
            # Check if it's a quota exceeded error
            if "quota" in error_msg or "exceeded" in error_msg or "429" in error_msg:
                logger.warning(f"Gemini API quota exceeded or rate limit hit: {e}")
                # Re-raise the exception to be handled at a higher level
                raise e
            else:
                logger.error(f"Error generating response with Gemini: {e}")
                # Fallback response
                return "The book does not provide details about this topic."

    async def generate_confidence_score(self, query: str, response: str, sources: List[Dict]) -> float:
        """
        Generate confidence score for the response based on source relevance
        """
        if not sources:
            return 0.0

        # Calculate average confidence from sources
        total_confidence = sum(source.get("confidence", 0.0) for source in sources)
        avg_confidence = total_confidence / len(sources)

        return avg_confidence

    async def process_query(self, query: str, mode: str = "full_book", selected_text: Optional[str] = None, top_k: int = 5) -> RAGResponse:
        """
        Process a query using the RAG pipeline
        """
        monitoring.log_user_action("system", "rag_query", {"query_length": len(query), "mode": mode})

        start_time = __import__('time').time()

        try:
            # Determine context based on mode, but validate selected_text
            # If mode is selected_text but no valid selected_text provided, use full book mode
            if mode == "selected_text" and selected_text and selected_text.strip():
                # Use only the provided selected text
                context = await self.retrieve_context_from_text(query, selected_text)
            else:
                # Use full book content (stored in vector database)
                # Log which mode is being used
                if mode == "selected_text" and (not selected_text or not selected_text.strip()):
                    logger.info("No valid selected text provided, using full_book mode instead of selected_text mode")
                context = await self.retrieve_context(query, top_k)

            # Generate response
            response_text = await self.generate_response(query, context)

            # Prepare references
            references = [item["source"] for item in context if "source" in item]

            # Log performance and chunk retrieval details
            execution_time = __import__('time').time() - start_time
            monitoring.log_performance("rag_process_query", execution_time)

            # Log chunk retrieval details
            logger.info(f"Retrieved {len(context)} chunks for query: '{query[:50]}...'")
            if hasattr(self, 'generative_model') and self.generative_model:
                logger.info(f"Generated response using Gemini model")

            return RAGResponse(
                response=response_text,
                sources=context,
                references=list(set(references))  # Remove duplicates
            )

        except ConnectionError as e:
            logger.error(f"Connection error during RAG query processing: {e}")
            monitoring.log_error(e, "Qdrant connection error")
            raise HTTPException(
                status_code=503,
                detail="Service temporarily unavailable due to vector database connection issues"
            )
        except Exception as e:
            error_msg = str(e).lower()
            # Check if it's a quota exceeded error and handle it appropriately
            if "quota" in error_msg or "exceeded" in error_msg or "429" in error_msg:
                logger.warning(f"Gemini API quota exceeded in process_query: {e}")
                raise HTTPException(
                    status_code=429,
                    detail=f"Gemini API quota exceeded: {str(e)}"
                )
            else:
                logger.error(f"Error during RAG query processing: {e}")
                monitoring.log_error(e, "RAG query processing failed")
                raise

    async def retrieve_context_from_text(self, query: str, text: str) -> List[Dict]:
        """
        Retrieve context from provided text (for selected-text mode)
        Make sure selected_text mode: cleans the text, chunks the text, embeds the text,
        ranks chunks by similarity, returns them sorted, but still uses Gemini to generate the final answer.
        """
        # Clean the provided text
        cleaned_text = await self.clean_text(text)

        # Chunk the provided text
        chunks = await self.chunk_text(cleaned_text)

        # Embed the query for similarity matching
        query_embedding = await self.embedding_service.generate_embedding(query)

        # Calculate relevance scores for each chunk
        context = []
        for i, chunk in enumerate(chunks):
            # Generate embedding for the chunk
            chunk_embedding = await self.embedding_service.generate_embedding(chunk)

            # Calculate similarity between query and chunk
            similarity = self._calculate_similarity(query_embedding, chunk_embedding)

            context.append({
                "chunk_id": f"selected_text_chunk_{i}",
                "content": chunk,
                "score": similarity,  # Using 'score' instead of 'confidence' to match requirements
                "source": "selected_text_input"  # Source metadata for selected text mode
            })

        # Sort by score in descending order (rank chunks by similarity)
        context.sort(key=lambda x: x["score"], reverse=True)

        return context

    async def store_selected_text_chunks(self, text: str, user_id: str, collection_name: str = None) -> List[str]:
        """
        Store selected text chunks in a separate Qdrant collection for future reference
        """
        if collection_name is None:
            collection_name = f"selected_text_{user_id}"

        # Clean and chunk the provided text
        cleaned_text = await self.clean_text(text)
        chunks = await self.chunk_text(cleaned_text)

        # Generate embeddings for the chunks
        embeddings = await self.embed_chunks(chunks)

        # Store in the selected text collection
        chunk_ids = await self.store_chunks(chunks, embeddings, collection_name)

        return chunk_ids

    async def ingest_content(self, content: str, collection_name: str = None, progress_tracker=None) -> Dict:
        """
        Ingest content into the RAG system (clean → chunk → embed → store flow)
        """
        if collection_name is None:
            collection_name = settings.QDRANT_COLLECTION_NAME

        try:
            # Clean the content
            if progress_tracker:
                progress_tracker.update_progress(status="cleaning", progress_percentage=10)

            cleaned_content = await self.clean_text(content)

            # Chunk the content
            if progress_tracker:
                progress_tracker.update_progress(status="chunking", progress_percentage=20)

            chunks = await self.chunk_text(cleaned_content)

            # Generate embeddings for chunks
            if progress_tracker:
                progress_tracker.update_progress(status="embedding", progress_percentage=30)

            embeddings = await self.embed_chunks(chunks)

            # Update progress during embedding (more granular but less frequent updates)
            if progress_tracker and len(chunks) > 0:
                # Update progress periodically during embedding (every 10% of chunks)
                update_interval = max(1, len(chunks) // 10)  # Update every 10% of chunks
                for i, chunk in enumerate(chunks):
                    if i % update_interval == 0 or i == len(chunks) - 1:  # Update every interval or at the end
                        progress = 30 + int((i / len(chunks)) * 40)  # 30% to 70% for embedding
                        progress_tracker.update_progress(
                            status="embedding",
                            progress_percentage=min(progress, 70)
                        )

            # Store in vector database
            if progress_tracker:
                progress_tracker.update_progress(status="storing", progress_percentage=70)

            chunk_ids = await self.store_chunks(chunks, embeddings, collection_name)

            # Final update
            if progress_tracker:
                progress_tracker.update_progress(
                    status="completed",
                    total_chunks=len(chunks),
                    progress_percentage=100
                )

            return {
                "chunks_processed": len(chunks),
                "chunk_ids": chunk_ids,
                "collection_name": collection_name
            }
        except Exception as e:
            if progress_tracker:
                progress_tracker.update_progress(
                    status="failed",
                    error_message=str(e),
                    progress_percentage=0
                )
            logger.error(f"Error during content ingestion: {e}")
            monitoring.log_error(e, "Content ingestion failed")
            raise